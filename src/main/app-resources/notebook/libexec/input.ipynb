{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## ewf-wfp-02-01-02 - Aggregated Land Surface Temperature Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated Land Surface Temperature Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'Aggregated Land Surface Temperature Time Series'),\n",
    "                ('abstract', 'Aggregated Land Surface Temperature Time Series'),\n",
    "                ('id', 'ewf-wfp-02-01-02')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = dict([('id', 'N_1'),\n",
    "            ('value', 'False'),\n",
    "            ('title', 'No Aggregation'),\n",
    "            ('abstract', 'No aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_3 = dict([('id', 'N_3'),\n",
    "            ('value', 'True'),\n",
    "            ('title', '30 Day Aggregation'),\n",
    "            ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_6 = dict([('id', 'N_6'),\n",
    "            ('value', 'False'),\n",
    "            ('title', '60 Day Aggregation'),\n",
    "            ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_9 = dict([('id', 'N_9'),\n",
    "            ('value', 'False'),\n",
    "            ('title', '90 Day Aggregation'),\n",
    "            ('abstract', 'Get a 90 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_12 = dict([('id', 'N_12'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '120 Day Aggregation'),\n",
    "             ('abstract', 'Get a 120 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_15 = dict([('id', 'N_15'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '150 Day Aggregation'),\n",
    "             ('abstract', 'Get a 150 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_18 = dict([('id', 'N_18'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '180 Day Aggregation'),\n",
    "             ('abstract', 'Get a 180 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_27 = dict([('id', 'N_27'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '270 Day Aggregation'),\n",
    "             ('abstract', 'Get a 270 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_36 = dict([('id', 'N_36'),\n",
    "             ('value', 'False'),\n",
    "             ('title', '360 Day Aggregation'),\n",
    "             ('abstract', 'Get a 360 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON((11.5030755518998 -11.1141633706909,41.0343255518998 -11.1141633706909,41.0343255518998 -34.9763656693858,11.5030755518998 -34.9763656693858,11.5030755518998 -11.1141633706909))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', 'SouthernAfrica'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = dict([('id', 'user'),\n",
    "             ('value', 'better-wfp-02-01-01'),\n",
    "             ('title', 'user'),\n",
    "             ('abstract', 'user to access catalog'),\n",
    "             ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = dict([('id', 'apikey'),\n",
    "                ('value', ''),\n",
    "                ('title', 'apikey'),\n",
    "                ('abstract', 'apikey to access catalog'),\n",
    "                ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = dict([('id', 'startdate'),\n",
    "                  ('value', '2020-02-25T00:00Z'),\n",
    "                  ('title', 'Start date'),\n",
    "                  ('abstract', 'Start date')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enddate = dict([('id', 'enddate'),\n",
    "                ('value', '2020-04-15T23:59Z'),\n",
    "                ('title', 'End date'),\n",
    "                ('abstract', 'End date')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_update = dict([('id', 'start_update'),\n",
    "                     ('value', '2020-05-15T00:00Z'),\n",
    "                     ('title', 'Start update'),\n",
    "                     ('abstract', 'Start update')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_update = dict([('id', 'stop_update'),\n",
    "                    ('value', '2020-05-25T00:00Z'),\n",
    "                    ('title', 'Stop update'),\n",
    "                    ('abstract', 'Stop update')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2process = dict([('id', 'n2process'),\n",
    "                  ('value', '1'),\n",
    "                  ('title', 'n of aggs to process'),\n",
    "                  ('abstract', 'n of aggs to process (-1 to process everthing)')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the MDOIS stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ('75C6ECAC64921C9042A194AB0F950039F870DED1',\n",
    "                    '7AF1D92ABBE578C1E8AD44E282930B22DDFDCF8C',\n",
    "                    'F2940F0C127A530B2EFF744CD4B8EF764FC6B14D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-wfp-02-01-01/search?format=atom&uid=75C6ECAC64921C9042A194AB0F950039F870DED1',\n",
    "                    'https://catalog.terradue.com/better-wfp-02-01-01/search?format=atom&uid=7AF1D92ABBE578C1E8AD44E282930B22DDFDCF8C',\n",
    "                    'https://catalog.terradue.com/better-wfp-02-01-01/search?format=atom&uid=F2940F0C127A530B2EFF744CD4B8EF764FC6B14D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/data/modis/ewf-wfp-02-01-01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aux folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove contents of a given folder\n",
    "# used to clean a temporary folder\n",
    "def rm_cfolder(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "def get_input_metadata (input_refs):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        \n",
    "        # since the search is by identifier \n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,title,startdate,enddate,wkt',creds='{}:{}'.format(user['value'],apikey['value']))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = pd.DataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "\n",
    "def get_formatted_date(date_str):\n",
    "    date = datetime.datetime.strftime(date_str, '%Y-%m-%dT00:00:00Z')\n",
    "    return date\n",
    "\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    first_date = get_formatted_date(first_date)\n",
    "    last_date = get_formatted_date(last_date)\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))\n",
    "            \n",
    "\n",
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None):\n",
    "    \n",
    "    dataset = None\n",
    "    \n",
    "    # creates directory if it doesnt exist yet\n",
    "    crop_directory = os.path.dirname(output_path)\n",
    "    if crop_directory is not '' and not os.path.exists(crop_directory):\n",
    "        os.makedirs(crop_directory)\n",
    "        \n",
    "    \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "        \n",
    "    else: # .tif\n",
    "        dataset = gdal.Open(input_image)\n",
    "        \n",
    "    \n",
    "    no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "    geo_t = dataset.GetGeoTransform()\n",
    "    polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "    envelope = polygon_ogr.GetEnvelope()\n",
    "    bounds = [envelope[0], envelope[2], envelope[1], envelope[3]]\n",
    "    \n",
    "    gdal.Warp(output_path, dataset, format=\"GTiff\", outputBoundsSRS='EPSG:4326', outputBounds=bounds, srcNodata=no_data_value, dstNodata=no_data_value, xRes=geo_t[1], yRes=-geo_t[5], targetAlignedPixels=True)\n",
    "    \n",
    "\n",
    "    \n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    \n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    \n",
    "    \n",
    "    if mask is not None and mask is not 0:\n",
    "\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    \n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def calc_max_matrix(mat1, mat2, no_data_value=None):\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "    \n",
    "    return np.where(mat1 > mat2, mat1, mat2)\n",
    "\n",
    "\n",
    "def matrix_sum(mat1, mat2, no_data_value=None):\n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "            \n",
    "            \n",
    "    msum = mat1 + mat2\n",
    "        \n",
    "    msum[(mat1 == 0)] = 0\n",
    "    msum[(mat2 == 0)] = 0\n",
    "        \n",
    "    return msum\n",
    "\n",
    "\n",
    "def calc_average(matrix_list, n_matrix, no_data_value=None):\n",
    "    if not matrix_list:\n",
    "        return 0\n",
    "    result = matrix_list[0]\n",
    "    for i in range(1, n_matrix):\n",
    "        result = matrix_sum(result, matrix_list[i], no_data_value)\n",
    "    \n",
    "    return np.divide(result, (n_matrix*1.00))\n",
    "\n",
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "\n",
    "def calc_aggregations(product_list, N_value, region_of_interest, product_type, is_it_to_crop=True):\n",
    "\n",
    "    mask_no_data_value = 0\n",
    "    max_values = 0\n",
    "    averages = 0\n",
    "    temp_list = []\n",
    "    no_data_value = None\n",
    "    \n",
    "    \n",
    "    geo_transform = None\n",
    "    projection = None\n",
    "    no_data_value = None\n",
    "    \n",
    "    print(type(product_list))\n",
    "    \n",
    "    for product_url in product_list:\n",
    "        \n",
    "        # uncompressed data\n",
    "        product = product_url.split('/')[-1]\n",
    "        print(product)\n",
    "        \n",
    "        cropped_product_path = os.path.join(temp_folder, 'crop_' + product)\n",
    "\n",
    "        try:\n",
    "            \n",
    "            if is_it_to_crop:\n",
    "                crop_image(product_url, region_of_interest, cropped_product_path, product_type)\n",
    "            else:\n",
    "                cropped_product_path = product_url\n",
    "\n",
    "            \n",
    "            # Read GeoTIFF as an array\n",
    "            dataset = gdal.Open(cropped_product_path)\n",
    "            product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "            no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "            print(no_data_value)\n",
    "            geo_transform = dataset.GetGeoTransform()\n",
    "            projection = dataset.GetProjection()\n",
    "            ## Create mask of no_data_values\n",
    "            if no_data_value is not None:\n",
    "                if isinstance(mask_no_data_value, int):\n",
    "                    mask_no_data_value = np.where(product_array == no_data_value, 1, 0)\n",
    "                else:\n",
    "                    temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "                    mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "            max_values = calc_max_matrix(max_values, product_array, no_data_value)\n",
    "            temp_list.append(product_array)\n",
    "            dataset = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Error processing the product ' + product + ': ' + str(e))\n",
    "            \n",
    "        if os.path.exists(cropped_product_path) and is_it_to_crop:\n",
    "            os.remove(cropped_product_path)\n",
    "    \n",
    "    averages = calc_average(temp_list, N_value, no_data_value)\n",
    "    \n",
    "    return max_values, averages, mask_no_data_value, geo_transform, projection, no_data_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_outputs(product_name, roi_name, first_date, last_date, averages, max_values, mask_no_data_value, image_format, product_count, projection, geo_transform, no_data_value):\n",
    "    filenames = []\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_averages_' + first_date + '_' + last_date + '.tif')\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_maxvalues_' + first_date + '_' + last_date + '.tif')\n",
    "    \n",
    "    #GDT_UInt16\n",
    "    \n",
    "    mask_no_data_value = None # Temp\n",
    "    write_output_image(filenames[0], averages, image_format,  gdal.GDT_UInt16, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    write_output_image(filenames[1], max_values, image_format, gdal.GDT_UInt16, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load metadata from catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Loading metadata from catalog' \n",
    "ciop.log('INFO', message)\n",
    "\n",
    "prods = get_input_metadata (input_references)\n",
    "prods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods = prods.sort_values(by='startdate', ascending=False)\n",
    "\n",
    "\n",
    "# N time steps\n",
    "nlist = [N_1['value'], N_3['value'], N_6['value'], N_9['value'], N_12['value'], N_15['value'], N_18['value'], N_27['value'], N_36['value']]\n",
    "nlist = [n == 'True' for n in nlist]\n",
    "nvalues = [1, 3, 6, 9, 12, 15, 18, 27, 36]\n",
    "\n",
    "num2process = int(n2process['value'])\n",
    "\n",
    "# py dict to group references by N.\n",
    "# For each N there is a list of subDataFrames with\n",
    "# the references for each aggregation\n",
    "input_references_by_N = {}\n",
    "\n",
    "for bl,nv in zip(nlist, nvalues):\n",
    "    \n",
    "    if bl: # if true for a given N\n",
    "        \n",
    "        N = nv\n",
    "        input_references_bins = []\n",
    "    \n",
    "        for i in range(len(prods)):\n",
    "    \n",
    "            if i > len(prods) - N: # break if its impossible to subset the dataframe\n",
    "                break\n",
    "        \n",
    "            d = prods.iloc[i:i+N]\n",
    "        \n",
    "            if num2process == -1: # append all possible combos\n",
    "                input_references_bins.append(d)\n",
    "            else:\n",
    "                if len(input_references_bins) < num2process:\n",
    "                    input_references_bins.append(d)\n",
    "    \n",
    "        input_references_by_N[nv] = input_references_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Number of aggs to process per N:' \n",
    "\n",
    "for nk in input_references_by_N.keys():\n",
    "    message = message + '\\nN' + str(nk) + ': ' + str(len(input_references_by_N[nk]))\n",
    "\n",
    "ciop.log('INFO', message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_wkt = regionOfInterest['value']\n",
    "\n",
    "global_wkt = 'POLYGON((-179.999 89.999, 179.999 89.999, 179.999 -89.999, -179.999 -89.999, -179.999 89.999))'\n",
    "\n",
    "\n",
    "polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "\n",
    "global_ogr = ogr.CreateGeometryFromWkt(global_wkt)\n",
    "\n",
    "is_it_to_crop = not(polygon_ogr.Contains(global_ogr))\n",
    "\n",
    "is_it_to_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_of_interest = regionOfInterest['value']\n",
    "name_of_region = nameOfRegion['value']\n",
    "\n",
    "for nk in input_references_by_N.keys():\n",
    "\n",
    "    input_references_bins = input_references_by_N[nk]\n",
    "    \n",
    "    for input_metada in input_references_bins:\n",
    "\n",
    "        # get start and last date from metadata\n",
    "        first_date = input_metada['startdate'].min().strftime('%Y-%m-%d')\n",
    "        last_date = input_metada['startdate'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "        print(first_date)\n",
    "        print(last_date)\n",
    "\n",
    "        # list of files\n",
    "\n",
    "        message = 'Computing aggregations' \n",
    "        ciop.log('INFO', message)\n",
    "\n",
    "\n",
    "        # works only if N and number of given images match\n",
    "        if nk == len(input_metada):\n",
    "            \n",
    "            N = nk\n",
    "            \n",
    "            file_list = [os.path.join(data_path, os.path.basename(enclosure).split('?')[0]) for enclosure in input_metada['enclosure']]\n",
    "        \n",
    "            #print(file_list)\n",
    "\n",
    "            max_values, averages, no_value, geo_transform, projection, no_data = calc_aggregations(file_list, N, region_of_interest, None, is_it_to_crop)\n",
    "            \n",
    "            #print(averages)\n",
    "                \n",
    "            message = 'Exporting result as geotiff' \n",
    "            ciop.log('INFO', message)\n",
    "\n",
    "            product_path_name = os.path.join(output_folder, 'LST' )\n",
    "\n",
    "            filenames = write_outputs(product_path_name, name_of_region, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, no_data)\n",
    "                \n",
    "            message = 'Writing properties file' \n",
    "            ciop.log('INFO', message)\n",
    "\n",
    "            for output_name in filenames:\n",
    "    \n",
    "                write_properties_file(output_name, datetime.datetime.strptime(first_date, \"%Y-%m-%d\").date(), datetime.datetime.strptime(last_date, \"%Y-%m-%d\").date(), region_of_interest)\n",
    "    \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"N value and number of images don't match\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_results:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(averages)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporay files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Removing temporary files' \n",
    "ciop.log('INFO', message)\n",
    "\n",
    "rm_cfolder(temp_folder)\n",
    "\n",
    "os.rmdir(temp_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ewf-wfp-02-01-02",
   "language": "python",
   "name": "env_ewf-wfp-02-01-02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
