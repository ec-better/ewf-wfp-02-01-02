{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## ewf-wfp-02-01-02 - Aggregated Land Surface Temperature Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated Land Surface Temperature Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'Aggregated Land Surface Temperature Time Series'),\n",
    "                ('abstract', 'Aggregated Land Surface Temperature Time Series'),\n",
    "                ('id', 'ewf-wfp-02-01-02')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_1 = dict([('id', 'N_1'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', 'No Aggregation'),\n",
    "                          ('abstract', 'No aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_3 = dict([('id', 'N_3'),\n",
    "                          ('value', 'True'),\n",
    "                          ('title', '30 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_6 = dict([('id', 'N_6'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '60 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 30 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_9 = dict([('id', 'N_9'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '90 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 90 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_12 = dict([('id', 'N_12'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '120 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 120 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_15 = dict([('id', 'N_15'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '150 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 150 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_18 = dict([('id', 'N_18'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '180 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 180 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_27 = dict([('id', 'N_27'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '270 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 270 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " N_36 = dict([('id', 'N_36'),\n",
    "                          ('value', 'False'),\n",
    "                          ('title', '360 Day Aggregation'),\n",
    "                          ('abstract', 'Get a 360 day aggregation')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON((11.5030755518998 -11.1141633706909,41.0343255518998 -11.1141633706909,41.0343255518998 -34.9763656693858,11.5030755518998 -34.9763656693858,11.5030755518998 -11.1141633706909))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', 'SouthernAfrica'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the MDOIS stack of products' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2015\n",
    "#input_identifiers = ('modis_lst_10d_2015-01-05.tif', 'modis_lst_10d_2015-01-15.tif', 'modis_lst_10d_2015-01-25.tif')\n",
    "\n",
    "# 2016\n",
    "#input_identifiers = ('modis_lst_10d_2016-01-05.tif', 'modis_lst_10d_2016-01-15.tif', 'modis_lst_10d_2016-01-25.tif')\n",
    "\n",
    "# 2017\n",
    "input_identifiers = ('modis_lst_10d_2017-01-05.tif', 'modis_lst_10d_2017-01-15.tif', 'modis_lst_10d_2017-01-25.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = tuple(['https://catalog.terradue.com/modis/search?format=atom&uid={0}'.format(pid) for pid in input_identifiers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/workspace/modis/outputs/output01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aux folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = 'temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove contents of a given folder\n",
    "# used to clean a temporary folder\n",
    "def rm_cfolder(folder):\n",
    "    #folder = '/path/to/folder'\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "#e.g. '2016-01-01'\n",
    "def get_formatted_date(date_str):\n",
    "    date = datetime.datetime.strftime(date_str, '%Y-%m-%dT00:00:00Z')\n",
    "    return date\n",
    "\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    first_date = get_formatted_date(first_date)\n",
    "    last_date = get_formatted_date(last_date)\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))\n",
    "            \n",
    "\n",
    "def crop_image(input_image, polygon_wkt, output_path, product_type=None):\n",
    "    \n",
    "    dataset = None\n",
    "    \n",
    "    # creates directory if it doesnt exist yet\n",
    "    crop_directory = os.path.dirname(output_path)\n",
    "    if crop_directory is not '' and not os.path.exists(crop_directory):\n",
    "        os.makedirs(crop_directory)\n",
    "        \n",
    "    \n",
    "    if input_image.startswith('ftp://') or input_image.startswith('http'):\n",
    "        try:\n",
    "            dataset = gdal.Open('/vsigzip//vsicurl/%s' % input_image)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif '.nc' in input_image:\n",
    "        dataset = gdal.Open('NETCDF:' + input_image + ':' + product_type)\n",
    "        \n",
    "    else: # .tif\n",
    "        dataset = gdal.Open(input_image)\n",
    "        \n",
    "    \n",
    "    no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "    geo_t = dataset.GetGeoTransform()\n",
    "    polygon_ogr = ogr.CreateGeometryFromWkt(polygon_wkt)\n",
    "    envelope = polygon_ogr.GetEnvelope()\n",
    "    bounds = [envelope[0], envelope[2], envelope[1], envelope[3]]\n",
    "    gdal.Warp(output_path, dataset, format=\"GTiff\", outputBoundsSRS='EPSG:4326', outputBounds=bounds, srcNodata=no_data_value, dstNodata=no_data_value, xRes=geo_t[1], yRes=-geo_t[5], targetAlignedPixels=True)\n",
    "    \n",
    "\n",
    "    \n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    \n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    \n",
    "    \n",
    "    if mask is not None and mask is not 0:\n",
    "        # TODO: check if output folder exists\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    \n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def calc_max_matrix(mat1, mat2, no_data_value=None):\n",
    "    \n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "    \n",
    "    return np.where(mat1 > mat2, mat1, mat2)\n",
    "\n",
    "\n",
    "def matrix_sum(mat1, mat2, no_data_value=None):\n",
    "    if no_data_value is not None:\n",
    "        if not isinstance(mat1, int):\n",
    "            mat1[(mat1 == no_data_value)] = 0\n",
    "        if not isinstance(mat2, int):\n",
    "            mat2[(mat2 == no_data_value)] = 0\n",
    "            \n",
    "            \n",
    "    msum = mat1 + mat2\n",
    "        \n",
    "    msum[(mat1 == 0)] = 0\n",
    "    msum[(mat2 == 0)] = 0\n",
    "        \n",
    "    return msum\n",
    "            \n",
    "            \n",
    "    #return mat1 + mat2\n",
    "\n",
    "def calc_average(matrix_list, n_matrix, no_data_value=None):\n",
    "    if not matrix_list:\n",
    "        return 0\n",
    "    result = matrix_list[0]\n",
    "    for i in range(1, n_matrix):\n",
    "        result = matrix_sum(result, matrix_list[i], no_data_value)\n",
    "    \n",
    "    return np.divide(result, (n_matrix*1.00))\n",
    "\n",
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "\n",
    "def calc_aggregations(product_list, N_value, region_of_interest, product_type):\n",
    "    mask_no_data_value = 0\n",
    "    max_values = 0\n",
    "    averages = 0\n",
    "    temp_list = []\n",
    "    no_data_value = None\n",
    "    \n",
    "    \n",
    "    # to check later (UnboundLocalError)\n",
    "    geo_transform = None\n",
    "    projection = None\n",
    "    no_data_value = None\n",
    "    \n",
    "    print(type(product_list))\n",
    "    \n",
    "    for product_url in product_list:\n",
    "        \n",
    "        # uncompressed data\n",
    "        product = product_url.split('/')[-1]\n",
    "        print(product)\n",
    "        \n",
    "        \n",
    "        \n",
    "        cropped_product_path = temp_folder + '/' + 'crop_' + product\n",
    "        #cropped_product_path = cropped_product_path.split('.nc')[0] + '.tif'\n",
    "        try:\n",
    "            \n",
    "            \n",
    "            crop_image(product_url, region_of_interest, cropped_product_path, product_type)\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            # Read GeoTIFF as an array\n",
    "            dataset = gdal.Open(cropped_product_path)\n",
    "            #pdb.set_trace()\n",
    "            product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "            no_data_value = dataset.GetRasterBand(1).GetNoDataValue()\n",
    "            print(no_data_value)\n",
    "            geo_transform = dataset.GetGeoTransform()\n",
    "            projection = dataset.GetProjection()\n",
    "            ## Create mask of no_data_values\n",
    "            if no_data_value is not None:\n",
    "                if isinstance(mask_no_data_value, int):\n",
    "                    mask_no_data_value = np.where(product_array == no_data_value, 1, 0)\n",
    "                else:\n",
    "                    temp_mask = np.where(product_array == no_data_value, 1, 0)\n",
    "                    mask_no_data_value = matrix_sum(mask_no_data_value, temp_mask)\n",
    "            max_values = calc_max_matrix(max_values, product_array, no_data_value)\n",
    "            temp_list.append(product_array)\n",
    "            dataset = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print('Error processing the product ' + product + ': ' + str(e))\n",
    "        if os.path.exists(cropped_product_path):\n",
    "            os.remove(cropped_product_path)\n",
    "    \n",
    "    averages = calc_average(temp_list, N_value, no_data_value)\n",
    "    \n",
    "    return max_values, averages, mask_no_data_value, geo_transform, projection, no_data_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_outputs(product_name, roi_name, first_date, last_date, averages, max_values, mask_no_data_value, image_format, product_count, projection, geo_transform, no_data_value):\n",
    "    filenames = []\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_averages_' + first_date + '_' + last_date + '.tif')\n",
    "    filenames.append(product_name + '_' + roi_name + '_N' + str(product_count) + '_maxvalues_' + first_date + '_' + last_date + '.tif')\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    #GDT_UInt16\n",
    "    \n",
    "    #write_output_image(filenames[0], averages, image_format,  gdal.GDT_Byte, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    #write_output_image(filenames[1], max_values, image_format, gdal.GDT_Byte, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    mask_no_data_value = None # Temp\n",
    "    write_output_image(filenames[0], averages, image_format,  gdal.GDT_UInt16, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    write_output_image(filenames[1], max_values, image_format, gdal.GDT_UInt16, mask_no_data_value, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folders\n",
    "#if not os.path.isdir(data_path):\n",
    "#    os.mkdir(data_path)\n",
    "\n",
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "if not os.path.isdir(temp_folder):\n",
    "    os.mkdir(temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-05\n",
      "2017-01-25\n",
      "['/workspace/modis/outputs/output01/modis_lst_10d_2017-01-05.tif', '/workspace/modis/outputs/output01/modis_lst_10d_2017-01-15.tif', '/workspace/modis/outputs/output01/modis_lst_10d_2017-01-25.tif']\n",
      "<type 'list'>\n",
      "modis_lst_10d_2017-01-05.tif\n",
      "0.0\n",
      "modis_lst_10d_2017-01-15.tif\n",
      "0.0\n",
      "modis_lst_10d_2017-01-25.tif\n",
      "0.0\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "if isinstance(input_references, str):\n",
    "    input_references = [input_references]\n",
    "\n",
    "region_of_interest = regionOfInterest['value']\n",
    "name_of_region = nameOfRegion['value']\n",
    "\n",
    "\n",
    "#first_date = '2017-01-01'\n",
    "#last_date = '2017-01-21'\n",
    "\n",
    "first_date = os.path.splitext(os.path.basename(input_identifiers[0].split('/')[-1]))[0].split('_')[-1]\n",
    "last_date = os.path.splitext(os.path.basename(input_identifiers[-1].split('/')[-1]))[0].split('_')[-1]\n",
    "\n",
    "print(first_date)\n",
    "print(last_date)\n",
    "\n",
    "#first_date = '2017-01-01'\n",
    "#last_date = '2017-01-21'\n",
    "\n",
    "#print(first_date)\n",
    "#print(last_date)\n",
    "\n",
    "\n",
    "# N time steps\n",
    "nlist = [N_1['value'], N_3['value'], N_6['value'], N_9['value'], N_12['value'], N_15['value'], N_18['value'], N_27['value'], N_36['value']]\n",
    "nlist = [n == 'True' for n in nlist]\n",
    "nvalues = [1, 3, 6, 9, 12, 15, 18, 27, 36]\n",
    "\n",
    "# list of files\n",
    "\n",
    "\n",
    "for bl,nv in zip(nlist, nvalues):\n",
    "    \n",
    "    # only works for selected N time steps\n",
    "    if bl:\n",
    "        \n",
    "        # works only if N and number of given images match\n",
    "        if nv == len(input_identifiers):\n",
    "            \n",
    "            N = nv\n",
    "            \n",
    "            # list of files with full path\n",
    "            file_list = [data_path + '/' + file_name.split('/')[-1] for file_name in input_identifiers]\n",
    "        \n",
    "            print(file_list)\n",
    "\n",
    "            max_values, averages, no_value, geo_transform, projection, no_data = calc_aggregations(file_list, N, region_of_interest, None)\n",
    "            \n",
    "            print(averages)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"N value and number of images don't match\")\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CHECK\n",
    "if check_results:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(averages)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averages[300,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CHECK\n",
    "\n",
    "# import georasters\n",
    "\n",
    "# Load data\n",
    "#raster = temp_folder + '/' + 'crop_regtdt201701d1.tif'\n",
    "#data = georasters.from_file(raster)\n",
    "\n",
    "# Plot data\n",
    "#data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves Aggregations to Geotiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#product_path_name = output_folder + '/' + 'LST'\n",
    "\n",
    "product_path_name = os.path.join(output_folder, 'LST' )\n",
    "\n",
    "#type(product_path_name)\n",
    "\n",
    "filenames = write_outputs(product_path_name, name_of_region, first_date, last_date, averages, max_values, no_value, 'GTiff', N, projection, geo_transform, no_data)\n",
    "\n",
    "# TODO\n",
    "#write_properties_file(interval_gpd, output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "if check_results:\n",
    "\n",
    "    import rasterio\n",
    "\n",
    "    from rasterio.plot import show\n",
    "\n",
    "    # Load data\n",
    "    #raster = output_folder + '/' + 'LST_SouthernAfrica_N3_maxvalues_'+first_date+'_'+last_date+'.tif'\n",
    "    raster = filenames[0]\n",
    "    data = rasterio.open(raster)\n",
    "\n",
    "    show(data)\n",
    "\n",
    "    #data = georasters.from_file(raster)\n",
    "\n",
    "    # Plot data\n",
    "    #data.plot()\n",
    "\n",
    "    #print(no_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write properties files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_name in filenames:\n",
    "    \n",
    "    write_properties_file(output_name, datetime.datetime.strptime(first_date, \"%Y-%m-%d\").date(), datetime.datetime.strptime(last_date, \"%Y-%m-%d\").date(), region_of_interest)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove temporay files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cfolder(temp_folder)\n",
    "\n",
    "os.rmdir(temp_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vacc-env2",
   "language": "python",
   "name": "vacc-env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
